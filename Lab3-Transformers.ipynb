{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fbefcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniforge3/envs/DLA/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e59a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21462898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Path configurations\n",
    "\n",
    "data_dir = './data' \n",
    "checkpoints_dir = './checkpoints'\n",
    "log_dir = './logs'\n",
    "\n",
    "batch = 64\n",
    "n_examples_tokenizer = 5\n",
    "n_examples_test = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4712fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear directories for data, checkpoints, and logs\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def clear(_dir):\n",
    "    if os.path.exists(_dir):\n",
    "        if os.path.isdir(_dir):\n",
    "            shutil.rmtree(_dir)\n",
    "        os.makedirs(_dir, exist_ok = True)\n",
    "\n",
    "# Uncomment the following lines to clear the directories\n",
    "\n",
    "#clear(data_dir)\n",
    "#clear(checkpoints_dir)\n",
    "#clear(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ce0d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 59463), started 0:53:03 ago. (Use '!kill 59463' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard setup\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (8530, 2), 'validation': (1066, 2), 'test': (1066, 2)}\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# Rotten Tomatoes dataset loading\n",
    "# Using the Cornell Movie Review Data for Rotten Tomatoes sentiment analysis\n",
    "# This dataset is available in the Hugging Face datasets library\n",
    "\n",
    "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "\n",
    "print(dataset.shape)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "data_loaders = []\n",
    "\n",
    "for split in dataset.keys():\n",
    "    data_loaders.append(DataLoader(dataset[split], batch_size = batch, shuffle = False))\n",
    "\n",
    "train, val, test = data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3eaa73",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Testing the Tokenizer with Examples\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")`**:  \n",
    "  Loads a pretrained tokenizer based on the DistilBERT model (uncased, meaning all text is lowercased).\n",
    "\n",
    "- **Loop over `n_examples_tokenizer`:**  \n",
    "  - Retrieves text samples from the training split of the `dataset`.  \n",
    "  - Applies the tokenizer to each example with:  \n",
    "    - `return_tensors=\"pt\"`: returns PyTorch tensors for integration with PyTorch models.  \n",
    "    - `truncation=True`: truncates inputs longer than the model‚Äôs max length.  \n",
    "    - `padding=True`: pads shorter inputs to the max length for batching.  \n",
    "  - Converts token IDs back to their string token representation and prints them, showing how text is split into tokens.\n",
    "\n",
    "- **Custom test string:**  \n",
    "  `\"Pippo was so silly at the beginning of the movie, he should had die since min. 1\"`  \n",
    "  Tokenized similarly, then prints:  \n",
    "  - The keys of the tokenizer output (`input_ids`, `attention_mask`, etc.).  \n",
    "  - The list of tokens after conversion from token IDs.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Why use a pretrained tokenizer:**  \n",
    "  Pretrained tokenizers have vocabulary and tokenization rules aligned with the pretrained language model, ensuring compatibility and optimal performance.\n",
    "\n",
    "- **Truncation and padding:**  \n",
    "  These ensure input sequences fit fixed sizes required by models and enable batch processing.\n",
    "\n",
    "- **Token IDs to tokens:**  \n",
    "  Converting IDs back to tokens helps inspect and debug tokenization, verifying the model input representation.\n",
    "\n",
    "- **Using PyTorch tensors:**  \n",
    "  Facilitates direct use in PyTorch model pipelines without extra conversion.\n",
    "\n",
    "- **Testing on custom text:**  \n",
    "  Demonstrates tokenizer behavior on arbitrary inputs, including token splits and special tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'conan', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarz', '##ene', '##gger', ',', 'jean', '-', 'cl', '##aud', 'van', 'dam', '##me', 'or', 'steven', 'sega', '##l', '.', '[SEP]']\n",
      "['[CLS]', 'the', 'gorgeous', '##ly', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co', '-', 'writer', '/', 'director', 'peter', 'jackson', \"'\", 's', 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', 'tolkien', \"'\", 's', 'middle', '-', 'earth', '.', '[SEP]']\n",
      "['[CLS]', 'effective', 'but', 'too', '-', 'te', '##pid', 'bio', '##pic', '[SEP]']\n",
      "['[CLS]', 'if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', ',', 'was', '##abi', 'is', 'a', 'good', 'place', 'to', 'start', '.', '[SEP]']\n",
      "['[CLS]', 'emerges', 'as', 'something', 'rare', ',', 'an', 'issue', 'movie', 'that', \"'\", 's', 'so', 'honest', 'and', 'keen', '##ly', 'observed', 'that', 'it', 'doesn', \"'\", 't', 'feel', 'like', 'one', '.', '[SEP]']\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "['[CLS]', 'pip', '##po', 'was', 'so', 'silly', 'at', 'the', 'beginning', 'of', 'the', 'movie', ',', 'he', 'should', 'had', 'die', 'since', 'min', '.', '1', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer with a few examples\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "for i in range(n_examples_tokenizer):\n",
    "    example = dataset[\"train\"][i][\"text\"]\n",
    "\n",
    "    ttt = tokenizer(example, return_tensors = \"pt\", truncation = True, padding = True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ttt[\"input_ids\"][0])\n",
    "    print(tokens)\n",
    "\n",
    "print()\n",
    "\n",
    "test_text = \"Pippo was so silly at the beginning of the movie, he should had die since min. 1\" #custom text to test\n",
    "\n",
    "ttt = tokenizer(test_text, return_tensors = \"pt\", truncation = True, padding = True) #test tokenized text\n",
    "#pt = pytorch (tensors)\n",
    "\n",
    "\n",
    "print(ttt.keys())\n",
    "\n",
    "tokens_ids = tokenizer.convert_ids_to_tokens(ttt[\"input_ids\"][0])\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c805",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Loading Pre-trained DistilBERT Model for Sequence Classification and Preparing Tokens\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`model = AutoModel.from_pretrained(\"distilbert-base-uncased\", num_labels=2)`**:  \n",
    "  Loads the DistilBERT model pretrained on a large corpus, adapted for sequence classification with **2 output labels** (e.g., 0 = negative review, 1 = positive review).  \n",
    "  *Note:* Usually, sequence classification models use `AutoModelForSequenceClassification`, but here `AutoModel` is used‚Äîlikely a simplified version or for feature extraction.\n",
    "\n",
    "- **`model.to(device)`**:  \n",
    "  Moves the model‚Äôs parameters to the specified device (CPU or GPU) for efficient computation.\n",
    "\n",
    "- **`tokens = {k: v.to(device) for k, v in ttt.items()}`**:  \n",
    "  Moves all token tensors (input IDs, attention masks, etc.) to the same device as the model, ensuring compatibility during forward pass.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Why load a pretrained model:**  \n",
    "  Pretrained models have learned rich language representations that improve downstream task performance and reduce training time.\n",
    "\n",
    "- **Setting `num_labels=2`:**  \n",
    "  Configures the model for binary classification tasks, such as sentiment analysis (good vs bad reviews).\n",
    "\n",
    "- **Device management:**  \n",
    "  Keeping model and input tensors on the same device avoids errors and accelerates inference/training.\n",
    "\n",
    "- **Token preparation:**  \n",
    "  Ensures that all inputs required by the model are ready and correctly formatted for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573412be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained DistilBERT model for sequence classification and pass the tokens\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\", num_labels = 2) #0 for bad reviews, 1 for good reviews\n",
    "model.to(device)\n",
    "\n",
    "tokens = {k: v.to(device) for k, v in ttt.items()} #move token tensors to device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de58ec8",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Feature Extraction Function Using a Pretrained Transformer Model\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`def compute_features(model, tokenizer, data_loader):`**  \n",
    "  Defines a function to compute feature embeddings for a dataset by passing text through a pretrained model.\n",
    "\n",
    "- **`model.eval()`**:  \n",
    "  Sets the model to evaluation mode, disabling dropout and other training-specific layers for consistent outputs.\n",
    "\n",
    "- **`features = []`**:  \n",
    "  Initializes a list to collect feature tensors batch-wise.\n",
    "\n",
    "- **`with torch.no_grad():`**  \n",
    "  Disables gradient calculation to save memory and computation since we only want inference outputs.\n",
    "\n",
    "- **Iterate over `data_loader`:**  \n",
    "  - Tokenizes the batch of texts with padding and truncation for uniform input lengths.  \n",
    "  - Moves tokens to the same device as the model.  \n",
    "  - Runs the model forward to get outputs (hidden states).  \n",
    "  - Extracts the embedding corresponding to the `[CLS]` token (first token) from the last hidden layer (`outputs.last_hidden_state[:, 0, :]`). This token typically summarizes the whole input sequence.  \n",
    "  - Detaches from computation graph and moves tensor back to CPU for accumulation.  \n",
    "  - Appends the batch‚Äôs embeddings to `features`.  \n",
    "  - Explicitly deletes intermediate variables and clears CUDA cache to free GPU memory, helpful for large datasets.\n",
    "\n",
    "- **Concatenate all batch features:**  \n",
    "  Combines the list of tensors along the batch dimension into a single feature tensor.\n",
    "\n",
    "- **Return `features`:**  \n",
    "  Outputs a tensor containing feature embeddings for the entire dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Feature extraction from `[CLS]` token:**  \n",
    "  The `[CLS]` embedding is commonly used as a fixed-size representation of the input sequence, useful for downstream tasks like classification or clustering.\n",
    "\n",
    "- **Evaluation mode and no grad:**  \n",
    "  Ensures deterministic outputs and improves efficiency by skipping gradient computations.\n",
    "\n",
    "- **Batch processing:**  \n",
    "  Efficiently handles large datasets in manageable chunks.\n",
    "\n",
    "- **Memory management:**  \n",
    "  Deleting intermediate variables and clearing cache helps prevent out-of-memory errors during processing on GPUs.\n",
    "\n",
    "- **Use case:**  \n",
    "  Extracted features can be used for training classifiers, similarity search, or as input to other ML algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function\n",
    "\n",
    "def compute_features(model, tokenizer, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(data_loader):\n",
    "            tokens = tokenizer(\n",
    "                batch[\"text\"],\n",
    "                padding = True,\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            )\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "            outputs = model(**tokens)\n",
    "\n",
    "            #extract the [CLS] token embedding (first token) from the last hidden state\n",
    "            #we take all items in batch (:) , the first token (0), and all hidden dimensions (:)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :].detach().cpu()\n",
    "\n",
    "            features.append(cls_embeds)\n",
    "\n",
    "            del tokens, outputs, cls_embeds \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    features = torch.cat(features, dim = 0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5751c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training and Evaluating an SVM Classifier on Extracted Features\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`clf = svm.SVC()`**:  \n",
    "  Initializes a Support Vector Machine (SVM) classifier with default settings.\n",
    "\n",
    "- **`features = compute_features(model, tokenizer, train)`**:  \n",
    "  Calls the previously defined function to extract feature embeddings from the training dataset using the pretrained model and tokenizer.\n",
    "\n",
    "- **`X = features.numpy()`**:  \n",
    "  Converts the extracted features from a PyTorch tensor to a NumPy array, as scikit-learn expects NumPy inputs.\n",
    "\n",
    "- **`y = np.array(dataset[\"train\"][\"label\"])`**:  \n",
    "  Retrieves the ground-truth labels for the training data as a NumPy array.\n",
    "\n",
    "- **`clf.fit(X, y)`**:  \n",
    "  Trains the SVM classifier on the extracted features (`X`) and labels (`y`).\n",
    "\n",
    "- **`print(\"acc:\", clf.score(X, y))`**:  \n",
    "  Prints the accuracy of the classifier on the training data by computing the fraction of correctly classified samples.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Why use SVM:**  \n",
    "  SVMs are effective classifiers, especially on smaller datasets or when the data is linearly separable in the feature space.\n",
    "\n",
    "- **Using pretrained features:**  \n",
    "  Instead of training a deep model end-to-end, pretrained transformer embeddings provide rich semantic features that simplify downstream classification tasks.\n",
    "\n",
    "- **Conversion to NumPy:**  \n",
    "  Scikit-learn requires NumPy arrays, so conversion from PyTorch tensors is necessary.\n",
    "\n",
    "- **Training and evaluation on same data:**  \n",
    "  Provides a quick sanity check on the classifier's fit, but ideally, evaluation should be done on a separate validation or test set to assess generalization.\n",
    "\n",
    "- **Pipeline advantage:**  \n",
    "  This approach leverages powerful pretrained representations with lightweight classical ML methods, often leading to efficient and effective solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8bf518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134/134 [00:03<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.816295427901524\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate small SVM classifier on the extracted features\n",
    "clf = svm.SVC()\n",
    "\n",
    "features = compute_features(model, tokenizer, train)\n",
    "\n",
    "X = features.numpy()\n",
    "y = np.array(dataset[\"train\"][\"label\"])\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"acc:\", clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fabeae",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Testing the SVM Classifier on Validation Data\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`features = compute_features(model, tokenizer, val)`**:  \n",
    "  Extracts feature embeddings for the validation dataset using the pretrained model and tokenizer.\n",
    "\n",
    "- **`X = features.numpy()`**:  \n",
    "  Converts the PyTorch tensor of features to a NumPy array for compatibility with scikit-learn.\n",
    "\n",
    "- **`y = np.array(dataset[\"validation\"][\"label\"])`**:  \n",
    "  Loads the true labels of the validation set as a NumPy array.\n",
    "\n",
    "- **`print(\"acc:\", clf.score(X, y))`**:  \n",
    "  Prints the accuracy of the SVM classifier on the validation set, showing how well the model generalizes.\n",
    "\n",
    "- **`y_pred = clf.predict(X)`**:  \n",
    "  Predicts labels for the validation features using the trained classifier.\n",
    "\n",
    "- **`mis_idxs = np.where(y_pred != y)[0]`**:  \n",
    "  Identifies indices of misclassified examples where predictions differ from true labels.\n",
    "\n",
    "- **Loop over misclassified examples (up to `n_examples_test`):**  \n",
    "  - Prints the original text from the validation set for qualitative error analysis.  \n",
    "  - Shows the predicted label for each misclassified example.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Testing on unseen data:**  \n",
    "  Evaluates the classifier's performance beyond the training set, measuring generalization.\n",
    "\n",
    "- **Misclassification analysis:**  \n",
    "  Reviewing misclassified samples helps diagnose model weaknesses and can guide improvements in preprocessing, feature extraction, or model selection.\n",
    "\n",
    "- **Feature consistency:**  \n",
    "  Using the same feature extraction process on validation data ensures fair comparison.\n",
    "\n",
    "- **Accuracy metric:**  \n",
    "  A straightforward measure of overall correct classification rate.\n",
    "\n",
    "- **Combining deep embeddings and classical ML:**  \n",
    "  Enables leveraging the strengths of pretrained models with simpler classifiers, often yielding efficient and interpretable pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bfe4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 45.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8142589118198874\n",
      "Text : made for teens and reviewed as such , this is recommended only for those under 20 years of age . . . and then only as a very mild rental .\n",
      "Predicted  : 0\n",
      "Text : imagine o . henry's <b>the gift of the magi</b> relocated to the scuzzy underbelly of nyc's drug scene . merry friggin' christmas !\n",
      "Predicted  : 0\n",
      "Text : nothing short of wonderful with its ten-year-old female protagonist and its steadfast refusal to set up a dualistic battle between good and evil .\n",
      "Predicted  : 0\n",
      "Text : those moviegoers who would automatically bypass a hip-hop documentary should give \" scratch \" a second look .\n",
      "Predicted  : 0\n",
      "Text : baby-faced renner is eerily convincing as this bland blank of a man with unimaginable demons within .\n",
      "Predicted  : 0\n"
     ]
    }
   ],
   "source": [
    "#Test the classifier\n",
    "\n",
    "features = compute_features(model, tokenizer, val)\n",
    "\n",
    "X = features.numpy()\n",
    "y = np.array(dataset[\"validation\"][\"label\"])\n",
    "\n",
    "print(\"acc:\", clf.score(X, y))\n",
    "\n",
    "y_pred  = clf.predict(X)\n",
    "mis_idxs = np.where(y_pred != y)[0]\n",
    "for i in mis_idxs[:n_examples_test]:\n",
    "    i = int(i)\n",
    "    print(\"Text :\", dataset[\"validation\"][i][\"text\"])\n",
    "    print(f\"Predicted  : { y_pred[i] }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1, 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Formatting dataset for tokenization\n",
    "\n",
    "ds = dataset.map(lambda example: tokenizer(example[\"text\"]), batched = True)\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338215a",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Load Pretrained DistilBERT Model for Sequence Classification\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`model_ds = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")`**:  \n",
    "  Loads the pretrained DistilBERT model specifically fine-tuned (or ready to fine-tune) for sequence classification tasks.  \n",
    "  By default, it assumes 2 labels (e.g., binary classification).\n",
    "\n",
    "- **`model_ds.to(device)`**:  \n",
    "  Moves the model to the specified device (`cpu` or `cuda`) to enable efficient computation.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Pretrained model usage:**  \n",
    "  Utilizing a pretrained transformer like DistilBERT leverages learned language representations, accelerating training and improving accuracy.\n",
    "\n",
    "- **Sequence classification head:**  \n",
    "  This model includes a classification head on top of the transformer, designed to output logits corresponding to class labels.\n",
    "\n",
    "- **Device placement:**  \n",
    "  Moving the model to the correct device is essential to ensure compatibility with input tensors and to utilize GPU acceleration if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_ds = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") #(n labels is 2 by default)\n",
    "model_ds.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c8ef7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Metrics and Training Setup for Fine-tuning DistilBERT\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **`dcwp = DataCollatorWithPadding(tokenizer=tokenizer)`**:  \n",
    "  Creates a data collator that dynamically pads batches to the max sequence length in the batch, ensuring consistent input sizes without excessive padding.\n",
    "\n",
    "- **`compute_metrics(eval_pred, metrics_dict)`**:  \n",
    "  Function to calculate evaluation metrics from model predictions:\n",
    "  - Extracts logits and true labels.\n",
    "  - Computes predicted classes by taking the argmax over logits.\n",
    "  - Computes each metric (accuracy, F1, precision, recall) using the passed metrics functions.\n",
    "  - Returns a dictionary of metric results.\n",
    "\n",
    "- **`metrics = {...}`**:  \n",
    "  Dictionary defining the evaluation metrics using standard sklearn metric functions with weighted averaging for multiclass balance.\n",
    "\n",
    "- **`training_args = TrainingArguments(...)`**:  \n",
    "  Configuration for the training process:\n",
    "  - Specifies output directories for checkpoints and logs.\n",
    "  - Sets learning rate (2e-5) suitable for fine-tuning transformers.\n",
    "  - Defines batch sizes for training (32) and evaluation (16).\n",
    "  - Runs for 5 epochs.\n",
    "  - Applies weight decay (0.01) to regularize training.\n",
    "  - Evaluates and saves the model at the end of each epoch.\n",
    "  - Loads the best model (by evaluation metric) after training.\n",
    "  - Enables logging every 10 steps and reports logs to TensorBoard.\n",
    "\n",
    "- **`trainer = Trainer(...)`**:  \n",
    "  Initializes the Hugging Face `Trainer` with the model, arguments, datasets, data collator, and metric computation function.\n",
    "\n",
    "- **`trainer.train()`**:  \n",
    "  Starts the fine-tuning process.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Data collator with padding:**  \n",
    "  Efficient batching requires uniform input shapes; dynamic padding minimizes unnecessary computation and memory use.\n",
    "\n",
    "- **Custom metrics function:**  \n",
    "  Allows monitoring multiple metrics during evaluation for a richer assessment of model performance.\n",
    "\n",
    "- **Weighted metrics:**  \n",
    "  Weighted averaging accounts for class imbalance by weighting per-class scores by support.\n",
    "\n",
    "- **Training arguments choices:**  \n",
    "  Learning rate, batch size, and epochs are typical fine-tuning values that balance performance and computational resources.\n",
    "\n",
    "- **Evaluation strategy:**  \n",
    "  Evaluating and saving at each epoch helps in early stopping and selecting the best checkpoint.\n",
    "\n",
    "- **Logging with TensorBoard:**  \n",
    "  Enables visualization and monitoring of training progress.\n",
    "\n",
    "- **Using Hugging Face Trainer:**  \n",
    "  Provides an easy-to-use, standardized API for training transformer models with built-in support for metrics, logging, checkpointing, and device management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab17178d-5028-47e3-af97-953e8de5aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1335/1335 00:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.369271</td>\n",
       "      <td>0.836773</td>\n",
       "      <td>0.836254</td>\n",
       "      <td>0.841095</td>\n",
       "      <td>0.836773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>0.349599</td>\n",
       "      <td>0.851782</td>\n",
       "      <td>0.851749</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.851782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.442669</td>\n",
       "      <td>0.845216</td>\n",
       "      <td>0.845101</td>\n",
       "      <td>0.846241</td>\n",
       "      <td>0.845216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.510387</td>\n",
       "      <td>0.845216</td>\n",
       "      <td>0.845216</td>\n",
       "      <td>0.845217</td>\n",
       "      <td>0.845216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.557311</td>\n",
       "      <td>0.843340</td>\n",
       "      <td>0.843336</td>\n",
       "      <td>0.843370</td>\n",
       "      <td>0.843340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1335, training_loss=0.20539348040627184, metrics={'train_runtime': 56.4881, 'train_samples_per_second': 755.026, 'train_steps_per_second': 23.633, 'total_flos': 580344848019696.0, 'train_loss': 0.20539348040627184, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics and Training setup\n",
    "\n",
    "dcwp = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred, metrics_dict):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis = -1)\n",
    "\n",
    "    metrics_results = {}\n",
    "    for metric in metrics_dict.keys():\n",
    "        metrics_results[metric] = metrics_dict[metric](preds, labels)\n",
    "\n",
    "    return metrics_results\n",
    "\n",
    "metrics = {\n",
    "    \"acc\": lambda preds, labels: (preds == labels).mean(),\n",
    "    \"f1\": lambda preds, labels: f1_score(labels, preds, average = \"weighted\"),\n",
    "    \"prec\": lambda preds, labels: precision_score(labels, preds, average = \"weighted\"),\n",
    "    \"rec\": lambda preds, labels: recall_score(labels, preds, average = \"weighted\")\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = checkpoints_dir + \"/fine_tuned_model\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = \"epoch\", #evaluate at the end of each epoch\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    logging_dir = log_dir + \"/fine_tuned_model\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 10,\n",
    "    report_to = \"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model_ds,\n",
    "    args = training_args,\n",
    "    train_dataset = ds[\"train\"],\n",
    "    eval_dataset = ds[\"validation\"],\n",
    "    data_collator = dcwp,\n",
    "    compute_metrics = lambda x: compute_metrics(x, metrics)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2d36c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è LoRa Fine-tuning Setup\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **Reload model:**  \n",
    "  Loads the pre-trained DistilBERT model for sequence classification with 2 output labels, then moves it to the device.\n",
    "\n",
    "- **LoRa Configuration (`LoraConfig`):**  \n",
    "  - `r = 8`: Rank of the low-rank decomposition (controls adaptation complexity).  \n",
    "  - `lora_alpha = 16`: Scaling factor for LoRa updates (commonly recommended value).  \n",
    "  - `target_modules = [\"q_lin\", \"v_lin\"]`: Applies LoRa only to the query and value linear layers in attention modules.  \n",
    "  - `lora_dropout = 0.1`: Dropout probability applied to LoRa layers for regularization.  \n",
    "  - `bias = \"none\"`: No bias parameters added during LoRa tuning.  \n",
    "  - `task_type = TaskType.SEQ_CLS`: Indicates this is a sequence classification task.\n",
    "\n",
    "- **Apply LoRa (`get_peft_model`):**  \n",
    "  Wraps the model to enable parameter-efficient LoRa fine-tuning by freezing most parameters except LoRa adapters.\n",
    "\n",
    "- **Print trainable parameters:**  \n",
    "  Confirms which parameters will be updated during training (typically only LoRa adapters).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **LoRa fine-tuning:**  \n",
    "  Enables efficient adaptation of large pretrained models by training a small subset of parameters, reducing compute and memory cost.\n",
    "\n",
    "- **Targeting `q_lin` and `v_lin`:**  \n",
    "  These are key projection layers in the attention mechanism, where adaptation has high impact on model performance.\n",
    "\n",
    "- **Using dropout in LoRa layers:**  \n",
    "  Helps reduce overfitting by regularizing the small set of trainable parameters.\n",
    "\n",
    "- **Freezing base model:**  \n",
    "  Keeps pretrained knowledge intact and speeds up training, only tuning lightweight LoRa components.\n",
    "\n",
    "- **Task-specific config:**  \n",
    "  Helps the PEFT framework apply correct assumptions and optimizations for sequence classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
     ]
    }
   ],
   "source": [
    "# LoRa fine-tuning\n",
    "\n",
    "# Redefine the model for LoRa fine-tuning\n",
    "model_ds = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 2)\n",
    "model_ds.to(device)\n",
    "\n",
    "lora_config = LoraConfig(    \n",
    "    r = 8, #LoRa rank decompostion\n",
    "    lora_alpha = 16, #Suggested value (no clue why)\n",
    "    target_modules = [\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model_ds = get_peft_model(model_ds, lora_config)\n",
    "model_ds.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de0929",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training the LoRa Fine-tuned Model\n",
    "\n",
    "### ‚úÖ Explanation\n",
    "\n",
    "- **Training Arguments (`TrainingArguments`):**  \n",
    "  - `output_dir`: Directory to save checkpoints (`checkpoints_dir + \"/lora_model\"`).  \n",
    "  - `learning_rate`: Set to **2e-5**, a typical low learning rate for fine-tuning transformer models.  \n",
    "  - `per_device_train_batch_size`: Batch size of **32** for training.  \n",
    "  - `per_device_eval_batch_size`: Batch size of **16** for evaluation.  \n",
    "  - `num_train_epochs`: Train for **3 epochs**, balancing training time and performance.  \n",
    "  - `weight_decay`: L2 regularization set to **0.01** to reduce overfitting.  \n",
    "  - `eval_strategy`: Evaluate the model at the end of each epoch.  \n",
    "  - `save_strategy`: Save model checkpoints at the end of each epoch.  \n",
    "  - `load_best_model_at_end`: Automatically load the best checkpoint (based on evaluation metric) after training.  \n",
    "  - `logging_dir`: Directory for TensorBoard logs (`log_dir + \"/lora_model\"`).  \n",
    "  - `logging_strategy` & `logging_steps`: Logs training metrics every **10 steps**.  \n",
    "  - `report_to`: Send logs to TensorBoard for visualization.\n",
    "\n",
    "- **Trainer Setup (`Trainer`):**  \n",
    "  - `model`: The LoRa-adapted DistilBERT model.  \n",
    "  - `args`: Training configuration from above.  \n",
    "  - `train_dataset` & `eval_dataset`: Training and validation datasets.  \n",
    "  - `data_collator`: Handles padding and batching (`dcwp` = DataCollatorWithPadding).  \n",
    "  - `compute_metrics`: Function to calculate evaluation metrics during training.\n",
    "\n",
    "- **Start training:**  \n",
    "  `trainer.train()` runs the fine-tuning process using the above settings.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Theory & Why\n",
    "\n",
    "- **Low learning rate:**  \n",
    "  Essential for stable fine-tuning of large pretrained models, especially when tuning a small subset of parameters (LoRa adapters).\n",
    "\n",
    "- **Few epochs:**  \n",
    "  LoRa fine-tuning typically converges faster due to fewer trainable parameters.\n",
    "\n",
    "- **Evaluation & checkpointing each epoch:**  \n",
    "  Enables monitoring progress and recovery of the best performing model.\n",
    "\n",
    "- **Weight decay:**  \n",
    "  Acts as regularization to prevent overfitting, improving generalization.\n",
    "\n",
    "- **TensorBoard logging:**  \n",
    "  Provides rich visual feedback on training metrics and helps diagnose issues.\n",
    "\n",
    "- **Using Trainer API:**  \n",
    "  Simplifies training loop management, evaluation, checkpointing, and integration with Hugging Face ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e023a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [801/801 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>0.468388</td>\n",
       "      <td>0.797373</td>\n",
       "      <td>0.797373</td>\n",
       "      <td>0.797378</td>\n",
       "      <td>0.797373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>0.424912</td>\n",
       "      <td>0.813321</td>\n",
       "      <td>0.813071</td>\n",
       "      <td>0.815007</td>\n",
       "      <td>0.813321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.420202</td>\n",
       "      <td>0.814259</td>\n",
       "      <td>0.814023</td>\n",
       "      <td>0.815864</td>\n",
       "      <td>0.814259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=801, training_loss=0.49919321578688985, metrics={'train_runtime': 19.5015, 'train_samples_per_second': 1312.208, 'train_steps_per_second': 41.074, 'total_flos': 354239374467936.0, 'train_loss': 0.49919321578688985, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = checkpoints_dir + \"/lora_model\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    logging_dir = log_dir + \"/lora_model\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 10,\n",
    "    report_to = \"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model_ds,\n",
    "    args = training_args,\n",
    "    train_dataset = ds[\"train\"],\n",
    "    eval_dataset = ds[\"validation\"],\n",
    "    data_collator = dcwp,\n",
    "    compute_metrics = lambda x: compute_metrics(x, metrics)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ed48-444d-47d4-bd8b-839a99e7996a",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Choose your Own Adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9129-ef2e-45f7-9e8f-baa697ccd91e",
   "metadata": {},
   "source": [
    "There are a *ton* of interesting and fun models on the HuggingFace hub. Pick one that does something interesting and adapt it in some way to a new task. Or, combine two or more models into something more interesting or fun. The sky's the limit.\n",
    "\n",
    "**Note**: Reach out to me by email or on the Discord if you are unsure about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c150bd36-6535-4724-a06d-a61632d3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
