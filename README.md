# ğŸ§  Deep Learning Applications Labs

This repository contains a series of hands-on labs exploring various deep learning paradigms, including supervised learning, reinforcement learning, and transformers. It was developed and tested locally on a high-performance workstation to ensure smooth execution and compatibility across environments.

---

## ğŸ“Œ Project Summary

The labs are structured as independent Jupyter notebooks and cover the following topics:

### Lab 1 â€“ **MLP & CNNs**
- Train MLPs and CNNs on MNIST and CIFAR-10
- Experiment with residual connections and knowledge distillation

### Lab 2 â€“ **Reinforcement Learning**
- Implement REINFORCE and Deep Q-Network (DQN) agents
- Use OpenAIâ€™s `gymnasium` environments for training and evaluation

### Lab 3 â€“ **Transformers**
- Perform sentiment analysis and feature extraction with Hugging Face Transformers (DistilBERT)
- Apply Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA

---

## ğŸ§ª System Configuration (Local Development)

All experiments were run and validated locally using the following setup:

- **GPU:** NVIDIA RTX 4070 Ti Super (16 GB VRAM)  
- **CPU:** AMD Ryzen 7 7800X3D  
- **RAM:** 32 GB DDR5 (Corsair Vengeance)  
- **Storage:** NVMe SSD Kingston

This configuration allows for fast model training and smooth interaction with large transformer models.

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/marcopibbes/DLA_labs
cd LabDLA
```

### 2. Set Up the Python Environment

> âš ï¸ **Note:** Due to compatibility issues between libraries (especially in Lab 2), you may need separate environments.

#### Using Conda (Recommended)

```bash

conda create -n dla_lab python=3.12.8 -y
conda activate dla_lab
conda install --file requirements.txt -c conda-forge -y


```

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ Lab1-CNNs.ipynb           # Supervised learning (MLPs, CNNs)
â”œâ”€â”€ Lab2-DRL.ipynb            # Reinforcement learning agents
â”œâ”€â”€ Lab3-Transformers.ipynb   # Transformers & PEFT
â”œâ”€â”€ BaseTrainingPipeline.py   # Core trainer class
â”œâ”€â”€ SLTrainingPipeline.py     # Supervised training pipeline
â”œâ”€â”€ RLTrainingPipeline.py     # REINFORCE trainer
â”œâ”€â”€ QLTrainingPipeline.py     # DQN trainer
â”œâ”€â”€ MLP.py                    # MLP and ResMLP definitions
â”œâ”€â”€ CNN.py                    # CNN and ResCNN definitions
â”œâ”€â”€ data/                     # Dataset storage (auto-downloaded)
â”œâ”€â”€ checkpoints/              # Saved models
â”œâ”€â”€ logs/                     # TensorBoard logs
â””â”€â”€ requirements.txt          # Dependency list
```

---

## ğŸ““ Running the Labs

1. Activate the appropriate Python environment.
2. Launch Jupyter:

```bash
jupyter lab  # or jupyter notebook
```

3. Open the desired notebook (`Lab1-CNNs.ipynb`, `Lab2-DRL.ipynb`, or `Lab3-Transformers.ipynb`) and start running the cells.

> ğŸ’¡ **Tip:**  
> If no checkpoints are available, make sure to comment out `pipeline.load()` and use `pipeline.train()` instead.

Each notebook also defines a `clear()` function to clean temporary data (checkpoints, logs, datasets).

---

## ğŸ“Š TensorBoard Support

To visualize logs and monitor training:

```bash
tensorboard --logdir=./logs
```

Then open [http://localhost:6006](http://localhost:6006) in your browser.

---

## ğŸ” Dependencies

All required packages are listed in `requirements.txt`, including:

- `torch`, `torchvision`, `transformers`, `tensorboard`
- `gymnasium`, `datasets`, `scikit-learn`, `matplotlib`, `pandas`
- Utilities for training, evaluation, and visualization

Some dependencies are version-pinned for compatibility and were installed via `conda-forge`.

---

## ğŸ§© Notes

- Expect first runs to download datasets and pretrained weights.
- Some visualizations (e.g., Lab 2) may require `pygame`.
- Tested across Python 3.9 and 3.12.8 environments.
